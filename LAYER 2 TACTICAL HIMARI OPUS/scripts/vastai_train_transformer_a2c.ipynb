{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HIMARI Layer 2 - Transformer-A2C Training\n",
                "\n",
                "**Run all cells in order to train the model with the fixed configuration.**\n",
                "\n",
                "Data: Downloads from Google Drive automatically"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Clone Repository & Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone the repository\n",
                "!git clone https://github.com/nimallansa937/HIMARI-OPUS-TACTICAL-L2-V1.0.git\n",
                "%cd HIMARI-OPUS-TACTICAL-L2-V1.0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install --upgrade pip -q\n",
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
                "!pip install numpy pandas scikit-learn scipy gdown tqdm -q\n",
                "print(\"✅ Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Download BTC Data from Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gdown\n",
                "import os\n",
                "\n",
                "# Create data directory\n",
                "os.makedirs(\"LAYER 2 TACTICAL HIMARI OPUS/data\", exist_ok=True)\n",
                "\n",
                "# Google Drive file ID from your share link\n",
                "FILE_ID = \"1_YMRsTCHjfsrqf63RI3xQ4jpehIsEaNW\"\n",
                "OUTPUT_PATH = \"LAYER 2 TACTICAL HIMARI OPUS/data/btc_5min_2020_2024.pkl\"\n",
                "\n",
                "if not os.path.exists(OUTPUT_PATH):\n",
                "    print(\"Downloading BTC data from Google Drive...\")\n",
                "    gdown.download(id=FILE_ID, output=OUTPUT_PATH, quiet=False)\n",
                "    print(f\"✅ Downloaded! File size: {os.path.getsize(OUTPUT_PATH) / 1e6:.1f} MB\")\n",
                "else:\n",
                "    print(f\"✅ Data file already exists: {OUTPUT_PATH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Verify Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "import numpy as np\n",
                "\n",
                "data_path = \"LAYER 2 TACTICAL HIMARI OPUS/data/btc_5min_2020_2024.pkl\"\n",
                "with open(data_path, 'rb') as f:\n",
                "    raw_data = pickle.load(f)\n",
                "\n",
                "print(f\"✅ Data loaded successfully!\")\n",
                "print(f\"Type: {type(raw_data)}\")\n",
                "\n",
                "if isinstance(raw_data, dict):\n",
                "    for k, v in raw_data.items():\n",
                "        if hasattr(v, 'shape'):\n",
                "            print(f\"  {k}: shape={v.shape}, dtype={v.dtype}\")\n",
                "        elif hasattr(v, '__len__'):\n",
                "            print(f\"  {k}: len={len(v)}\")\n",
                "elif hasattr(raw_data, 'shape'):\n",
                "    print(f\"Shape: {raw_data.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Setup Training Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import logging\n",
                "\n",
                "# Add source to path\n",
                "sys.path.insert(0, 'LAYER 2 TACTICAL HIMARI OPUS')\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(\n",
                "    level=logging.INFO,\n",
                "    format='%(asctime)s | %(levelname)s | %(message)s',\n",
                "    datefmt='%Y-%m-%d %H:%M:%S'\n",
                ")\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "# Check GPU\n",
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Prepare Data & Create Environments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.transformer_a2c import TransformerA2CConfig\n",
                "from src.environment.transformer_a2c_env import TransformerA2CEnv, WalkForwardSplitter, TransformerEnvConfig\n",
                "\n",
                "# Extract features and prices\n",
                "if isinstance(raw_data, dict):\n",
                "    if 'features' in raw_data and 'prices' in raw_data:\n",
                "        features = raw_data['features']\n",
                "        prices = raw_data['prices']\n",
                "    elif 'data' in raw_data:\n",
                "        features = raw_data['data']\n",
                "        prices = raw_data.get('prices', raw_data.get('close', None))\n",
                "    else:\n",
                "        keys = list(raw_data.keys())\n",
                "        features = raw_data[keys[0]]\n",
                "        prices = raw_data[keys[1]] if len(keys) > 1 else None\n",
                "else:\n",
                "    features = raw_data\n",
                "    prices = None\n",
                "\n",
                "# Convert to numpy\n",
                "features = np.array(features, dtype=np.float32)\n",
                "if prices is not None:\n",
                "    prices = np.array(prices, dtype=np.float32)\n",
                "else:\n",
                "    print(\"⚠️ No prices found, generating from first feature column\")\n",
                "    prices = features[:, 0] if features.ndim > 1 else features\n",
                "\n",
                "print(f\"Features shape: {features.shape}\")\n",
                "print(f\"Prices shape: {prices.shape}\")\n",
                "\n",
                "# Create train/val/test splits\n",
                "splitter = WalkForwardSplitter(\n",
                "    features, \n",
                "    prices,\n",
                "    train_ratio=0.7,\n",
                "    val_ratio=0.15,\n",
                "    test_ratio=0.15\n",
                ")\n",
                "\n",
                "# Feature dimension\n",
                "feature_dim = features.shape[1] if features.ndim > 1 else 1\n",
                "\n",
                "# Create environments\n",
                "env_config = TransformerEnvConfig(\n",
                "    context_length=100,\n",
                "    feature_dim=feature_dim,\n",
                ")\n",
                "\n",
                "train_env, val_env, test_env = splitter.create_envs(config=env_config)\n",
                "print(f\"✅ Environments created!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Configure & Start Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.training.transformer_a2c_trainer import train_transformer_a2c\n",
                "\n",
                "# Training config (WITH ALL FIXES APPLIED)\n",
                "config = TransformerA2CConfig(\n",
                "    input_dim=feature_dim,\n",
                "    hidden_dim=256,\n",
                "    num_heads=8,\n",
                "    num_layers=4,\n",
                "    context_length=100,\n",
                "    entropy_coef=0.07,      # FIXED: increased from 0.01\n",
                "    max_steps=100_000,\n",
                "    val_frequency=25_000,\n",
                "    patience=3,\n",
                ")\n",
                "\n",
                "print(\"=\" * 70)\n",
                "print(\"Training with FIXED configuration:\")\n",
                "print(f\"  ✅ entropy_coef: {config.entropy_coef} (was 0.01)\")\n",
                "print(f\"  ✅ Look-ahead bias: FIXED\")\n",
                "print(f\"  ✅ Entropy decay: ENABLED (0.07 → 0.021)\")\n",
                "print(f\"  ✅ Action distribution logging: ENABLED\")\n",
                "print(\"=\" * 70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# START TRAINING\n",
                "result = train_transformer_a2c(\n",
                "    train_env=train_env,\n",
                "    val_env=val_env,\n",
                "    config=config,\n",
                "    device=\"cuda\",\n",
                "    output_dir=\"./output/transformer_a2c\",\n",
                "    use_wandb=False,\n",
                ")\n",
                "\n",
                "if result:\n",
                "    print(\"\\n\" + \"=\" * 70)\n",
                "    print(\"✅ TRAINING COMPLETE!\")\n",
                "    print(f\"Best checkpoint: {result['path']}\")\n",
                "    print(f\"Best validation Sharpe: {result['val_sharpe']:.4f}\")\n",
                "    print(\"=\" * 70)\n",
                "else:\n",
                "    print(\"Training completed but no best checkpoint found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Download Checkpoints (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List checkpoints\n",
                "!ls -la ./output/transformer_a2c/*.pt 2>/dev/null || echo \"No checkpoints yet\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compress checkpoints for download\n",
                "!tar -czvf transformer_a2c_checkpoints.tar.gz ./output/transformer_a2c/\n",
                "print(\"\\n✅ Checkpoints compressed to: transformer_a2c_checkpoints.tar.gz\")\n",
                "print(\"Download this file from the Jupyter file browser.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}